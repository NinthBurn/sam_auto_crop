{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfEI6Jh2ACNN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Based on the following:\n",
    "Understanding how and why it works: <br>\n",
    "> https://www.youtube.com/watch?v=0Fpb8TBH0nM <br>\n",
    "\n",
    "Example for using SAM and processing its output: <br>\n",
    "> https://github.com/IDEA-Research/Grounded-Segment-Anything/blob/main/grounded_sam.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the environment for first time use\n",
    "## Dependencies\n",
    "To start off, you need to install <font color='green'>__CUDA__</font>:<br> \n",
    "> __https://developer.nvidia.com/cuda-downloads__\n",
    "\n",
    "After you have installed the <font color='green'>__CUDA toolkit__</font>, you will have to install the latest version of <font color='ff8000'>__PyTorch__</font>.\n",
    "> __https://pytorch.org/get-started/locally/__\n",
    "<br>\n",
    "This site will generate the command you need to install the package.\n",
    "\n",
    "In your Python environment, run the following command to install missing dependencies:\n",
    "> `pip install diffusers transformers accelerate scipy safetensors`\n",
    "\n",
    "## AI Models\n",
    "This script uses two models:\n",
    "- [Segment Anything](https://github.com/facebookresearch/segment-anything) is a strong segmentation model. But it need prompts (like boxes/points) to generate masks.\n",
    "- [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is a strong zero-shot detector which enable to generate high quality boxes and labels with free-form text.\n",
    "\n",
    "## Script setup\n",
    "Create a folder on your system. For the sake of this tutorial, we will call it `mainFolder` <br>\n",
    "Once you are in `mainFolder`, copy the Grounding DINO repository by running the command\n",
    "> __<font color='#ff8000'>git clone https://github.com/IDEA-Research/GroundingDINO.git</font>__\n",
    "\n",
    "In the same folder, clone the SAM repository:\n",
    "> __<font color='#ff8000'>git clone https://github.com/facebookresearch/segment-anything.git</font>__\n",
    "\n",
    "You also need to download a checkpoint for SAM, which has to be placed in `mainFolder`.\n",
    "> __https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth__\n",
    "\n",
    "Once you have all the files, you must do the following using your Python environment:\n",
    ">1. Navigate to `mainFolder\\GroundingDINO` and run the command `pip install -e .`\n",
    ">\n",
    ">3. Navigate to `mainfolder\\segment_anything` and run the command `pip install -e .`\n",
    "\n",
    "If there are no errors, then you have successfully installed the required dependencies.\n",
    "\n",
    "The script will look for images to process in `mainFolder\\content\\input` , and will generate the output in `mainFolder\\content\\output` . It will mirror the folder structure from the input folder.\n",
    "<br>\n",
    "Once you have finished a batch of files, <font color='red'>make sure to remove them from the input folder</font>, as they will get processed again otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Et4Gn3wsjpK"
   },
   "source": [
    "# Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object(s) to search for. Examples: \"rock, card, plane\" etc.\n",
    "# Value type: string\n",
    "search_for = 'rock; card'\n",
    "\n",
    "# Set whether or not to stop at the first matching object.\n",
    "# Value type: boolean\n",
    "only_first_match = False\n",
    "\n",
    "# Print additional information (used for debugging)\n",
    "# Value type: boolean\n",
    "debug_print = True\n",
    "\n",
    "def dprint(message: str):\n",
    "    if debug_print:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just run everything from top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z1XkItFZsjpP"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageSequence\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# diffusers\n",
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# If you have multiple GPUs, you can set the GPU to use here.\n",
    "# The default is to use the first GPU, which is usually GPU 0.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn-wtoWusjpQ"
   },
   "source": [
    "# Load Grounding DINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtBLz5PxsjpQ"
   },
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file)\n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "# Use this command for evaluate the Grounding DINO model\n",
    "# Or you can download the model by yourself\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yHLIcDUsjpS"
   },
   "source": [
    "# Load SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQurkKgUsjpT"
   },
   "outputs": [],
   "source": [
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=sam_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZqbbAuKsjpU"
   },
   "source": [
    "# Run Grounding DINO for detection\n",
    "Runs the Grounding DINO model on a given image.\n",
    "\n",
    "Returns an array with processed boxes for said image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TIvSsoFJsjpU"
   },
   "outputs": [],
   "source": [
    "def run_grounding_dino(image):\n",
    "    # search for objects of interest\n",
    "    TEXT_PROMPT = search_for\n",
    "    BOX_TRESHOLD = 0.3\n",
    "    TEXT_TRESHOLD = 0.25\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=groundingdino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNf3ke3usjpV"
   },
   "source": [
    "# Run the segmentation model\n",
    "Runs the Segment Anything model on a given image with annotated boxes.\n",
    "\n",
    "Returns an array with the masks identified by the model for a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DTd_zG_isjpW"
   },
   "outputs": [],
   "source": [
    "def run_sam(image_source, boxes):\n",
    "    # set image for SAM\n",
    "    sam_predictor.set_image(image_source)\n",
    "\n",
    "    # box: normalized box xywh -> unnormalized xyxy\n",
    "    H, W, _ = image_source.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2])\n",
    "    masks, _, _ = sam_predictor.predict_torch(\n",
    "            point_coords = None,\n",
    "            point_labels = None,\n",
    "            boxes = transformed_boxes,\n",
    "            multimask_output = False,\n",
    "        )\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qswj_70usjpX"
   },
   "source": [
    "# Image Crop\n",
    "Crops the image using the masks.\n",
    "\n",
    "Returns the result image in OpenCV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HQ8ICOOAsjpX"
   },
   "outputs": [],
   "source": [
    "# use for multiple objects\n",
    "def crop_image(image_source, masks):\n",
    "    # extract masks\n",
    "    image_mask_pil = Image.fromarray(masks[0][0].cpu().numpy())\n",
    "    for mask in masks:\n",
    "        current_mask = Image.fromarray(mask[0].cpu().numpy())\n",
    "      \n",
    "    # combine masks into one\n",
    "    image_mask_pil.paste(current_mask, (0, 0), mask=current_mask)\n",
    " \n",
    "    # convert mask & image to np arrays\n",
    "    mask_cv = np.array(image_mask_pil.convert('RGB'))\n",
    "    image_cv = np.array(Image.fromarray(image_source).convert('RGB'))\n",
    "\n",
    "    # Convert RGB to BGR for cv2\n",
    "    mask_cv = mask_cv[:, :, ::-1].copy()\n",
    "\n",
    "    # result image with black background\n",
    "    masked = cv2.bitwise_and(image_cv, mask_cv)\n",
    "\n",
    "    tmp = cv2.cvtColor(masked, cv2.COLOR_BGR2GRAY)\n",
    "    _, alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
    "    b, g, r = cv2.split(masked)\n",
    "    rgba = [r,g,b, alpha]\n",
    "\n",
    "    # result image with transparent background\n",
    "    masked_transparent = cv2.merge(rgba,4)\n",
    "\n",
    "    return masked_transparent\n",
    "\n",
    "# use for a single object\n",
    "def crop_image_single(image_source, masks):\n",
    "    # extract masks\n",
    "    image_mask_pil = Image.fromarray(masks[0][0].cpu().numpy())\n",
    " \n",
    "    # convert mask & image to np arrays\n",
    "    mask_cv = np.array(image_mask_pil.convert('RGB'))\n",
    "    image_cv = np.array(Image.fromarray(image_source).convert('RGB'))\n",
    "\n",
    "    # Convert RGB to BGR for cv2\n",
    "    mask_cv = mask_cv[:, :, ::-1].copy()\n",
    "\n",
    "    # result image with black background\n",
    "    masked = cv2.bitwise_and(image_cv, mask_cv)\n",
    "\n",
    "    tmp = cv2.cvtColor(masked, cv2.COLOR_BGR2GRAY)\n",
    "    _, alpha = cv2.threshold(tmp,0,255,cv2.THRESH_BINARY)\n",
    "    b, g, r = cv2.split(masked)\n",
    "    rgba = [r,g,b, alpha]\n",
    "\n",
    "    # result image with transparent background\n",
    "    masked_transparent = cv2.merge(rgba,4)\n",
    "\n",
    "    return masked_transparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_hRrUR4qw9R"
   },
   "source": [
    "# Image Processing\n",
    "Runs the model (GroundingDINO + SAM + crop) for a given image.\n",
    "\n",
    "Returns the result image in OpenCV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0PiKEJtDq1F-"
   },
   "outputs": [],
   "source": [
    "if only_first_match:\n",
    "    cropping_function = crop_image_single\n",
    "else:\n",
    "    cropping_function = crop_image\n",
    "\n",
    "def process_image(image_path):\n",
    "    image_source, image = load_image(image_path)\n",
    "    boxes = run_grounding_dino(image)\n",
    "    if not (boxes.numel()):\n",
    "        raise Exception('No object matching description was found')\n",
    "      \n",
    "    masks = run_sam(image_source, boxes)\n",
    "    final_image = cropping_function(image_source, masks)\n",
    "    return final_image\n",
    "\n",
    "def load_frame(frame):\n",
    "    transform = T.Compose(\n",
    "                [\n",
    "                    T.RandomResize([800], max_size=1333),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ]\n",
    "            )\n",
    "    frame = frame.convert('RGB')\n",
    "    image = np.asarray(frame)\n",
    "    image_transformed, _ = transform(frame, None)\n",
    "    return image, image_transformed\n",
    "\n",
    "def process_gif(image_path):  \n",
    "    with Image.open(image_path) as im:\n",
    "        frame_count = 1\n",
    "        filename, file_extension = os.path.splitext(image_path)\n",
    "        file_extension = file_extension.lower()\n",
    "        result_path = filename.replace('input', 'output', 1) + '\\\\'\n",
    "\n",
    "        # create a folder having the file's name\n",
    "        if not os.path.exists(result_path):\n",
    "           os.makedirs(result_path)\n",
    "                    \n",
    "        # extract all frames from gif\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(im)]\n",
    "        dprint(f'Current file is a gif, having {len(frames)} frames')\n",
    "        \n",
    "        # process frame by frame\n",
    "        for frame in frames:\n",
    "            image, image_transformed = load_frame(frame)\n",
    "            \n",
    "            boxes = run_grounding_dino(image_transformed)\n",
    "            if not (boxes.numel()):\n",
    "                dprint(f'Object not found in frame {str(frame_count)}')\n",
    "                frame_count += 1\n",
    "                continue\n",
    "                          \n",
    "            masks = run_sam(image, boxes)\n",
    "            final_image = cropping_function(image, masks)   \n",
    "            frame_path = result_path + '\\\\' + str(frame_count) + '.png'\n",
    "            \n",
    "            dprint(f'Saving frame {str(frame_count)} to {frame_path}')\n",
    "            cv2.imwrite(frame_path, final_image)\n",
    "            frame_count += 1\n",
    "\n",
    "        print(f'Saved processed frames in {result_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCdBkQFdrBeU"
   },
   "source": [
    "# Batch Processing\n",
    "Process all images from the input folder and save the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "qU50mCUqrLsX",
    "outputId": "97930c6b-2d6a-40bc-8b5f-bd8899f0ab47"
   },
   "outputs": [],
   "source": [
    "image_extensions = ['.png', '.jpg', '.jpeg', '.webp', '.gif']\n",
    "\n",
    "def process_image_or_navigate_folder(file, in_path):\n",
    "    current_path = in_path + '\\\\' + file\n",
    "    filename, file_extension = os.path.splitext(current_path)\n",
    "    \n",
    "    try:\n",
    "        # current path is a folder\n",
    "        if (os.path.isdir(current_path)):\n",
    "            for file_name in os.listdir(current_path):\n",
    "                process_image_or_navigate_folder(file_name, current_path)\n",
    "\n",
    "        # or an image\n",
    "        elif file_extension.lower() in image_extensions:\n",
    "            dprint(f'Currently in {in_path}')\n",
    "            print(f'Processing image {file}')\n",
    "\n",
    "            if file_extension == '.gif':\n",
    "                process_gif(current_path)\n",
    "                    \n",
    "            else:\n",
    "                file = file.replace(file_extension, '.png')\n",
    "        \n",
    "                result = process_image(current_path)\n",
    "                result_path = in_path.replace('input', 'output', 1) + '\\\\'\n",
    "                \n",
    "                if not os.path.exists(result_path):\n",
    "                    os.makedirs(result_path)\n",
    "                    \n",
    "                result_path += file\n",
    "                print(f'Saving processed file to {result_path}')\n",
    "                cv2.imwrite(result_path, result)\n",
    "            \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "\n",
    "def process_all():\n",
    "    if only_first_match:\n",
    "        dprint('Script will save only the first matching object')\n",
    "    else:\n",
    "        dprint('Script will save all matching objects')\n",
    "\n",
    "    dprint('Searching for: ' + search_for)\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    input_directory = current_directory + '\\\\content\\\\input'\n",
    "    output_directory = current_directory + '\\\\content\\\\output'\n",
    "\n",
    "    dprint(f'Files will be read from {input_directory}')\n",
    "    dprint(f'Processed images will be saved to {output_directory}')\n",
    "    \n",
    "    if not os.path.exists(input_directory):\n",
    "        os.makedirs(input_directory)\n",
    "        print('Input directory has been created. Add images and restart the script.')\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(input_directory):\n",
    "        process_image_or_navigate_folder(file_name, input_directory)\n",
    "\n",
    "process_all()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "206a3a76d28f48e8bbb75569029d6324": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "297db673b6cc47b69948bd0a19eab513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_206a3a76d28f48e8bbb75569029d6324",
      "placeholder": "​",
      "style": "IPY_MODEL_7e1345bfdfca45aaac5d2f6d5ef3fe12",
      "value": " 4/6 [00:23&lt;00:12,  6.13s/it]"
     }
    },
    "30a789e51224486a9fe01689d710a222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "394a9e9cffd048efbaab82bc8c52b2a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a351721d4e94cfaabf719fc8ac9470f",
       "IPY_MODEL_73a8f57fde8547e393ffbc1d660a8fa1",
       "IPY_MODEL_297db673b6cc47b69948bd0a19eab513"
      ],
      "layout": "IPY_MODEL_d51fe8b6363b406597514d8f313c17d0"
     }
    },
    "6a351721d4e94cfaabf719fc8ac9470f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b5fd04a296d4cd1b1dfb7ba0fe0da4b",
      "placeholder": "​",
      "style": "IPY_MODEL_ab603ae6967347048c11e2ae4f708591",
      "value": "Loading pipeline components...:  67%"
     }
    },
    "73a8f57fde8547e393ffbc1d660a8fa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f971954d16da46ca9730ac7f779be0a9",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30a789e51224486a9fe01689d710a222",
      "value": 4
     }
    },
    "7e1345bfdfca45aaac5d2f6d5ef3fe12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b5fd04a296d4cd1b1dfb7ba0fe0da4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab603ae6967347048c11e2ae4f708591": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d51fe8b6363b406597514d8f313c17d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f971954d16da46ca9730ac7f779be0a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
